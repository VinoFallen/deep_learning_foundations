# Deep Leaning Foundations

## Folder 1 : Neural Network From Scratch 
Basic 1 layer NN, 2 neurons and 2 feature input



## Folder 2 : NN: Zero to Hero - Micrograd
Andrej Karpathy's Micrograd - implementation <br>
Micrograd is a very small autograd engine. It implements Backpropagation using a custom class Value which can only use scalar values.<br>
This is used to build a small 2 layer MLP (Multi Layer Perceptron), which showcases a general idea of how PyTorch/Tensorflow works.


## Folder 3 : NN: Zero to Hero - Makemore