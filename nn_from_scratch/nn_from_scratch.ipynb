{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09680b7-773c-4552-be3d-ca079bc655cd",
   "metadata": {},
   "source": [
    "# 1. Coding a NEURON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5adb21-125d-4659-bc80-6893be2854fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# global sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Neuron class\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0,1])\n",
    "bias = 4\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2,3])\n",
    "print(n.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66db04-d3a7-461d-a46a-fe1b396365a4",
   "metadata": {},
   "source": [
    "# 2. Coding a NEURAL NETWORK : FEEDFORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2fa61b-a659-45cb-9868-7523757f6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "# Neural network class\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([0, 1])\n",
    "        self.b = 0\n",
    "        self.h1 = Neuron(self.w, self.b)\n",
    "        self.h2 = Neuron(self.w, self.b)\n",
    "        self.o1 = Neuron(self.w, self.b)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "        \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d7c5c-1e16-4177-818b-e3233872eb6c",
   "metadata": {},
   "source": [
    "# 3. Coding a MSE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "107d91d3-076c-4813-bc03-50de9f7c8e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "print(mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebaa03b-20ae-4f85-995b-bd671b8610d3",
   "metadata": {},
   "source": [
    "# 4. Coding a Complete NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dea051-9d2f-44cd-b994-2b70b1e4b6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss : 0.5940343471666747\n",
      "Epoch 10 loss : 0.4969870128864339\n",
      "Epoch 20 loss : 0.40084048481003876\n",
      "Epoch 30 loss : 0.3017214070296164\n",
      "Epoch 40 loss : 0.22197807476755024\n",
      "Epoch 50 loss : 0.16804885559984922\n",
      "Epoch 60 loss : 0.1306801628879839\n",
      "Epoch 70 loss : 0.10406967078242298\n",
      "Epoch 80 loss : 0.08476357324355466\n",
      "Epoch 90 loss : 0.0704012283732785\n",
      "Epoch 100 loss : 0.059430702279837266\n",
      "Epoch 110 loss : 0.050840471841010496\n",
      "Epoch 120 loss : 0.04396541819629707\n",
      "Epoch 130 loss : 0.03836111164672457\n",
      "Epoch 140 loss : 0.03372448359029147\n",
      "Epoch 150 loss : 0.029843838854953217\n",
      "Epoch 160 loss : 0.026567112051840796\n",
      "Epoch 170 loss : 0.0237815867521913\n",
      "Epoch 180 loss : 0.021400965786462466\n",
      "Epoch 190 loss : 0.019357202420262464\n",
      "Epoch 200 loss : 0.017595374829455102\n",
      "Epoch 210 loss : 0.01607044234671037\n",
      "Epoch 220 loss : 0.014745119512149735\n",
      "Epoch 230 loss : 0.013588395441133938\n",
      "Epoch 240 loss : 0.012574427440578234\n",
      "Epoch 250 loss : 0.011681664102236918\n",
      "Epoch 260 loss : 0.010892124257132658\n",
      "Epoch 270 loss : 0.010190794183296143\n",
      "Epoch 280 loss : 0.009565121857340738\n",
      "Epoch 290 loss : 0.009004593834018929\n",
      "Epoch 300 loss : 0.008500383061911698\n",
      "Epoch 310 loss : 0.008045057257175454\n",
      "Epoch 320 loss : 0.007632338445994013\n",
      "Epoch 330 loss : 0.007256905288326516\n",
      "Epoch 340 loss : 0.006914230848829034\n",
      "Epoch 350 loss : 0.006600449533393566\n",
      "Epoch 360 loss : 0.0063122479033910365\n",
      "Epoch 370 loss : 0.006046774975503121\n",
      "Epoch 380 loss : 0.005801568395200011\n",
      "Epoch 390 loss : 0.005574493534433959\n",
      "Epoch 400 loss : 0.005363693116514541\n",
      "Epoch 410 loss : 0.005167545425726403\n",
      "Epoch 420 loss : 0.004984629529933843\n",
      "Epoch 430 loss : 0.0048136962447889245\n",
      "Epoch 440 loss : 0.004653643810574132\n",
      "Epoch 450 loss : 0.004503497447888548\n",
      "Epoch 460 loss : 0.0043623921153668555\n",
      "Epoch 470 loss : 0.0042295579188702075\n",
      "Epoch 480 loss : 0.004104307723195377\n",
      "Epoch 490 loss : 0.003986026599229123\n",
      "Epoch 500 loss : 0.003874162805575144\n",
      "Epoch 510 loss : 0.0037682200571535532\n",
      "Epoch 520 loss : 0.0036677508766341446\n",
      "Epoch 530 loss : 0.003572350859814019\n",
      "Epoch 540 loss : 0.0034816537147865185\n",
      "Epoch 550 loss : 0.0033953269582366378\n",
      "Epoch 560 loss : 0.0033130681714558683\n",
      "Epoch 570 loss : 0.003234601734501171\n",
      "Epoch 580 loss : 0.003159675969979179\n",
      "Epoch 590 loss : 0.003088060638733278\n",
      "Epoch 600 loss : 0.0030195447386674274\n",
      "Epoch 610 loss : 0.002953934565389617\n",
      "Epoch 620 loss : 0.0028910519995727184\n",
      "Epoch 630 loss : 0.00283073299112943\n",
      "Epoch 640 loss : 0.0027728262146593163\n",
      "Epoch 650 loss : 0.002717191874294869\n",
      "Epoch 660 loss : 0.0026637006391679527\n",
      "Epoch 670 loss : 0.0026122326933350776\n",
      "Epoch 680 loss : 0.002562676886218468\n",
      "Epoch 690 loss : 0.0025149299715062005\n",
      "Epoch 700 loss : 0.00246889592406153\n",
      "Epoch 710 loss : 0.002424485325764059\n",
      "Epoch 720 loss : 0.002381614812381072\n",
      "Epoch 730 loss : 0.0023402065745751965\n",
      "Epoch 740 loss : 0.002300187907022723\n",
      "Epoch 750 loss : 0.002261490800363881\n",
      "Epoch 760 loss : 0.0022240515713526886\n",
      "Epoch 770 loss : 0.002187810527132816\n",
      "Epoch 780 loss : 0.0021527116600509153\n",
      "Epoch 790 loss : 0.0021187023698404\n",
      "Epoch 800 loss : 0.002085733210375514\n",
      "Epoch 810 loss : 0.0020537576585158705\n",
      "Epoch 820 loss : 0.0020227319028416285\n",
      "Epoch 830 loss : 0.00199261465032439\n",
      "Epoch 840 loss : 0.0019633669491941146\n",
      "Epoch 850 loss : 0.0019349520264511978\n",
      "Epoch 860 loss : 0.0019073351386392015\n",
      "Epoch 870 loss : 0.0018804834346404864\n",
      "Epoch 880 loss : 0.001854365829386335\n",
      "Epoch 890 loss : 0.0018289528874878588\n",
      "Epoch 900 loss : 0.0018042167158954054\n",
      "Epoch 910 loss : 0.0017801308647842134\n",
      "Epoch 920 loss : 0.0017566702359441018\n",
      "Epoch 930 loss : 0.0017338109980221492\n",
      "Epoch 940 loss : 0.001711530508030669\n",
      "Epoch 950 loss : 0.0016898072385895951\n",
      "Epoch 960 loss : 0.0016686207104226948\n",
      "Epoch 970 loss : 0.001647951429672455\n",
      "Epoch 980 loss : 0.0016277808296389002\n",
      "Epoch 990 loss : 0.0016080912165841144\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        # data is a (n x 2) numpy array, n = # of samples in the dataset\n",
    "        # all_y_true is a numpy array with n elements\n",
    "\n",
    "        learning_rate = 0.2\n",
    "        epochs = 1000\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # Do a feedforward (we'll need the values for later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "                # Calculate the derivatives\n",
    "                # Naming -> d_L_d_w1 is partial L / partial w1\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                #Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "                # Neuron h2\n",
    "                self.w3 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "                # Neuron o1\n",
    "                self.w5 -= learning_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learning_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learning_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "                # Calculate total loss\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                print(f\"Epoch {epoch} loss : {loss}\")\n",
    "\n",
    "# Define Dataset\n",
    "data = np.array([[-2,-1], [25,6], [17,4], [-15,-6]])\n",
    "all_y_trues = np.array([1, 0, 0, 1])\n",
    "\n",
    "# Train our neural network\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4277e-b214-4a9c-bcc4-3199eac8ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.969\n",
      "Frank: 0.041\n"
     ]
    }
   ],
   "source": [
    "# make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F (Around these values)\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2406f8-0345-4361-85a7-f775384fa9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
